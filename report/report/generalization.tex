% !TEX root = report.tex
%============================================================================================
\newpage
\section{Robust PCA with known rank: a block coordinate descent approach}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Motivation}
In some applications we may have some prior information about the low rank component of an observed matrix. For example, in computer vision, if we were to extract the background from different frames of a video, then it is natural to consider each video frame as a long vector (by stacking its columns). The background that we are recovering is then a single vector, which means that the associated component in the matrix collecting the frames is of rank 1. Therefore, it is a very natural question to ask whether we can utilize this additional rank information to derive faster or more scalable algorithms while retaining the performance guarantees of Robust PCA.



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Equivalent formulation of Robust PCA with rank information}

We first show the intuitive fact that within the robust PCA framework, the same probability guarantee will still hold when additional information on the problem is incorporated as constraints the optimization problem. Then we derive a block-coordinate descent algorithm for the case when rank information is known.
\begin{prop}
\label{prop:restriction prob}Let $M=L_{0}+S_{0}$ , $rank(L_{0})\le r$ and $(L_0,S_0)$
satisfy the Robust PCA assumptions. Then with high probability, the
following problems are equivalent:
%
\begin{eqnarray}
J_{1} & =\min_{L,S} & \|L\|_{*}+\lambda\|S\|_{1}\label{eq:general}\\
 & s.t. & M=L+S\nonumber
\end{eqnarray}
%
\begin{eqnarray}
J_{2} & =\min_{L,S} & \|L\|_{*}+\lambda\|S\|_{1}\label{eq:restricted}\\
 & s.t. & M=L+S\nonumber \\
 &  & \text{and }\;T(L,S)\text{ holds}\nonumber
\end{eqnarray}
%
where $T(L,S)$ are conditions on $(L,S)$ that $(L_{0},S_{0})$ also
satisfies.
\end{prop}

\begin{proof}
We use subscript to denote the optimizer for $J_{1}$ and $J_{2}$ respectively.
With high probability, $(L_{1},S_{1})=(L_{0},S_{0})$. Now, over the event of $(L_{1},S_{1})=(L_{0},S_{0})$, $(L_{1},S_{1})$ is also a feasible solution for (\ref{eq:restricted}). Since $J_{1}\le J_{2}$ always, now $(L_{1},S_{1})$ achieve the bound for (\ref{eq:restricted}).
Note that it should also be unique because otherwise it would contradict with the recovery of (\ref{eq:general}).
\end{proof}

Now we apply Proposition (\ref{prop:restriction prob}) to the case when the rank information is known and derive a block coordinate descent algorithm.

Consider the problem
\begin{alignat}{3}
J_{3}
&= &&\min_{L, S}  && \|L\|_{*} + \lambda \|S\|_{1} \notag \\
&&&\text{s.t.} && M=L+S, \ \text{rank}(L)\le r \notag \\
&= &&\min_{L, S, p_i, q_i, \mu_i} && \|L\|_{*} + \lambda\| M - L \|_{1} \notag \\
&&& \text{s.t.} && L = \sum_{i=1}^{r} \mu_{i}p_{i}q_{i}^{T}, \  \|p_{i}\|_{2}=1, \ \|q_{i}\|_{2}=1, \ \mu_{i}\ge0 \notag \\
&= && \min_{\mu_{i}, p_{i}, q_{i}} && \sum_{i=1}^{r} \mu_{i} + \lambda \|M - \sum_{i=1}^{r} \mu_{i}p_{i}q_{i}^{T}\|_{1} \notag \\
&&& \text{s.t.} && \|p_{i}\|_{2} = 1, \ \|q_{i}\|_{2} = 1, \mu_{i} \ge 0 \notag \\
& = && \min_{\mu_{i}, p_{i}, q_{i}} && \sum_{i=1}^{r} |\mu_{i}-0| + \lambda \|M - \sum_{i=1}^{r} \mu_{i} p_{i} q_{i}^{T}\|_{1} \label{eq:rank form} \\
&&& \text{s.t.} && \|p_{i}\|_{2} = 1, \ \|q_{i}\|_{2} = 1, \ \mu_{i} \ge 0 \notag
\end{alignat}

Note that this formulation allows us to optimize over $\mu_{i},p_{i}q_{i}$ sequentially. For each of these steps, the associated problem is a weighted median problem where an efficient algorithm is known. By the Proposition~\ref{prop:restriction prob}, we know that the formulation of~\eqref{eq:rank form} can recover the original pair $(L_{0},S_{0})$ with high probability.


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Simplification using $\ell_{1}$ heuristic}


%------------------------------------------------------------------------------
\subsubsection{Introduction}

Recall that the nuclear norm is used in the PCP scheme because we would like as a heuristic to recover the low rank component from gross random noise. The nuclear norm is used because it is a heuristic for penalizing high rank matrices (the nuclear norm encourages sparsity of the vector of singular values). Now, we consider the case when we have additional information about the rank of the low-rank matrix~$L$. Assume the rank of the matrix is known. Therefore, it is natural to introduce the following heuristic.
%
\begin{eqnarray}
E^{*} & = & \min_{\{p_{j}\}\{q_{j}\},1\le j\le r}\|M-\sum_{j=1}^{r}p_{j}q_{j}^{T}\|_{1}\label{heu}
\end{eqnarray}



%------------------------------------------------------------------------------
\subsubsection{Performance guarantee for the $\ell_1$ heuristic}

For this new heuristic, we provide some performance guarantee for the case when the noise is bounded. One is a result for deterministic case
and the other is for the random case. They are as follows.

\begin{prop}
Let $M=S+\sum_{i=1}^{r}p_{i}q_{i}^{T}$ and $\frac{2}{\epsilon}\|S\|_{1}\le\|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|_{1}$. Then, the estimate $\hat{L}$ recovered from~\eqref{heu} satisfies
\begin{eqnarray*}
\frac{\|\sum_{i=1}^{r}p_{i}q_{i}^{T}-\hat{L}\|_{1}}{\|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|_{1}} & \le & \epsilon
\end{eqnarray*}
\end{prop}
\begin{proof}
Suppose not, then
\begin{eqnarray*}
\|S\|_{1} & \ge & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}+S-\hat{L}\|_{1}\\
 & \ge & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}-\hat{L}\|_{1}-\|S\|_{1}\\
 & > & \epsilon\|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|_{1}-\|S\|_{1}
\end{eqnarray*}
which contradicts the assumption that
\begin{eqnarray*}
\frac{2}{\epsilon}\|S\|_{1} & > & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|
\end{eqnarray*}
\end{proof}
%This gives a contraditcion, which is,
%\begin{eqnarray*}
%\frac{2}{\epsilon}\|S\|_{1} & > & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|
%\end{eqnarray*}

\begin{prop}
Let $M=\sum_{i=1}^{r}p_{i}q_{i}^{T}+S$, where $S_{i,j}\sim Uniform(-x_{s},x_{s})$,
$(p_{i})_{j}\sim Uniform(-x_{p},x_{p})$, $(q_{i})_{j}\sim Uniform(-x_{q},x_{q})$, where all random variables are independent. With $|S|=k$ such that
$\lim_{n\to\infty}\frac{k^{2}}{n}$, then we have,
\[
\lim_{n \to \infty} P \left( \frac{ \|\sum_{i=1}^{r} p_{i} q_{i}^{T} - \hat{L} \|_{1} }{ \| \sum_{i=1}^{r} p_{i} q_{i}^{T} \|_{1} } > \epsilon \right) = 0
\]
\end{prop}

\begin{proof}
Let $E$ be the error event that $\frac{\|\sum_{i=1}^{r}p_{i}q_{i}^{T}-\hat{L}\|_{1}}{\|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|_{1}}>\epsilon$.
If error occurs,
\begin{eqnarray*}
kx_{s} & \ge & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}+S-\hat{L}\|_{1}\\
 & \ge & \|\sum_{i=1}^{r}p_{i}q_{i}^{T}-\hat{L}\|_{1}-\|S\|_{1}\\
 & \ge & \epsilon\|\sum_{i=1}^{r}p_{i}q_{i}^{T}\|_{1}-kx_{s}\\
 & \ge & \epsilon\sqrt{\sum_{l_{1}l_{2}}(\sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{2}})^{2}}-k_{s}
\end{eqnarray*}


Thus,
\begin{eqnarray*}
Pr(E)
&\le & Pr \left( \left( \frac{2kx_{s}}{\epsilon} \right)^{2} \ge \sum_{l_{1}l_{2}} \left( \sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{2}} \right)^{2} \right)\\
&\le & Pr \left( \left( \frac{2kx_{s}}{\epsilon} \right)^{2} \ge \sum_{l_{1}=1}^{n} \left( \sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{1}} \right)^{2} \right)\\
&= & Pr \left( \frac{1}{n} \left( \frac{2kx_{s}}{\epsilon} \right)^{2} \ge \frac{1}{n} \sum_{l_{1}=1}^{n} \left( \sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{1}} \right)^{2} \right)
\end{eqnarray*}


Moreover, as $E(\sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{1}})^{2})=\frac{r}{3}x_{p}^{2}x_{q}^{2}$,
by the law of large numbers, $\frac{1}{n}\sum_{l_{1}=1}^{n}(\sum_{i=1}^{r}(p_{i})_{l_{1}}(q_{i})_{l_{1}})^{2})\to\frac{r}{3}x_{p}^{2}x_{q}^{2}$.
Thus, since $\frac{1}{n}(\frac{2kx_{s}}{\epsilon})^{2}\to0$. This
gives $Pr(E)\to0$ as $n\to\infty$.
\end{proof}

However, we know that the $\ell_1$ heuristic cannot work well in the case of unbounded noise. This can be seen from the following example. Say, for $n=100$, 
\begin{alignat*}{2}
L_{0}&=\begin{bmatrix} 1\\ 1\\ \vdots \\ 1\end{bmatrix} \begin{bmatrix} 1 & 1 & \dots & 1 \end{bmatrix}, &\qquad \qquad
S_{0} &= \begin{bmatrix} 10^{9} & 0 & \dots & 0\\  0 & 0 & \dots & 0\\ \vdots & \vdots &  & \vdots\\ 0 & 0 & \dots & 0
\end{bmatrix}
\end{alignat*}
%
then by the $\ell_{1}$ heuristic, we would get 
\begin{alignat*}{2}
\hat{L}_{0}&=\begin{bmatrix} 10^9\\ 1\\ \vdots \\ 1\end{bmatrix} \begin{bmatrix} 1 & 10^{-9} & \dots & 10^{-9} \end{bmatrix}, &\qquad \qquad
\hat{S}_{0} &= \begin{bmatrix} 1 & 0 & \dots & 0\\  0 & 1 & \dots & 1\\ \vdots & \vdots &  & \vdots\\ 0 & 1 & \dots & 1
\end{bmatrix}
\end{alignat*}
%
which deviates grossly from the original pair. However, Robust PCA can achieve exact recovery even in this case because it penalize the nuclear norm of~$L$.



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithms derivation}

Note that the form of~\eqref{eq:rank form} and~\eqref{heu} is similar. One can generalize the method for~$r=1$ in~\eqref{heu}
to higher dimensions for both~\eqref{eq:rank form} and~\eqref{heu}. For simplicity we restrict our discussion to $r=1$ in~\eqref{heu}.

Let $M=(a_{i,j})\in R^{m \times n}$. We now employ the block-coordinate descent approach to solve this problem. Note that
%
\begin{eqnarray}
\min_{p} & \|M-pq^{T}\|_{1} & =\sum_{i=1}^{m}\min_{t}\biggl(\sum_{j=1}^{n}|a_{i,j}-tq_{j}|\biggr)\\
 &  & =\sum_{i=1}^{m}\min_{t}\biggl(\sum_{j=1}^{n}|q_{j}\|t-\frac{a_{i,j}}{q_{j}}|\biggr)\nonumber
\end{eqnarray}
\begin{eqnarray}
\min_{q} & \|M-pq^{T}\|_{1} & =\sum_{j=1}^{n}\min_{t}\biggl(\sum_{i=1}^{m}|a_{i,j}-tp_{i}|\biggr)\\
 &  & =\sum_{j=1}^{n}\min_{t}\biggl(\sum_{j=1}^{n}|p_{i}\|t-\frac{a_{i,j}}{p_{i}}|\biggr)\nonumber
\end{eqnarray}


And for solving the subproblem of finding
\[
\min_{t}\sum_{k=1}^{k_{0}}c_{i}|t-d_{i}|
\]
where $c_{i}\ge0$ is basically finding the weighted median and can be done by the WMH algorithm given in Algorithm~\ref{wmhalgo}, with has a complexity of $O(k_{0}\log k_{0})$, mostly through sorting the sequence.

\begin{algorithm}[h]
\begin{enumerate}
\item We first sort $\vec{\ensuremath{d}}$ s.t. $d_{i_{1}}\le d_{i_{2}}\le...\le d_{i_{k_{0}}}$
\item We then find $k^{'}$s.t.
\begin{eqnarray*}
\sum_{\theta=1}^{k^{'}-1}c_{i_{\theta}} & \le & \sum_{\theta=k^{'}}^{k_{0}}c_{i_{\theta}}\\
\sum_{i=1}^{k^{'}}c_{i_{\theta}} & \ge & \sum_{i=k^{'}+1}^{k_{0}}c_{i_{\theta}}
\end{eqnarray*}
\item We then set $t^{*}$to be $d_{i_{k^{'}}}$
\end{enumerate}
\caption{WMH $(k_{0},\vec{c},\vec{d})$}
\label{wmhalgo}
\end{algorithm}
% 
This WMH algorithm is optimal in finding $t$. This is shown by using the property of sub-differential of $\|\cdot\|_{1}$ and by noting that
$0\in\partial(\sum_{k=1}^{k_{0}}c_{i}|t^{*}-d_{i}|)$. 

Now we are ready to state the power iteration method to solve the rank-1 optimization problem through the power iteration heuristic given in Algorithm~\ref{poweritalg}.

\begin{algorithm}[h]
Repeat
\begin{enumerate}
\item $p_{i}\leftarrow wmh(n,abs(q),M(i,:)./q)$ for each i
\item $q_{j}\leftarrow wmh(n,abs(p),M(:,j)./p)$ for each j
\end{enumerate}
Until stopping criterion is met
\caption{Poweriteration($M$)}
\label{poweritalg}
\end{algorithm}

It is important to point out that while the~$\ell_1$ heuristic enjoys some recovery guarantees, we have no guarantee that the power iteration heuristic will solve the problem. However, we have found that the heuristic performs well in practice, at least for the rank 1 case. 



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Sensitivity to $\lambda$}

Note that in the Robust PCA framework, the parameter $\lambda$ is explicit stated as $n^{-1/2}$, and when $\lambda$ is too large or too small, it would significantly affect the recovery. However, in the case that the rank information is explicitly stated, the effect of $\lambda$ is different. As we have seen in previous section, when the value of $\lambda$ is large, it corresponds to the $\ell_{1}$ heuristic and there is some guarantee for recovery. It is natural to ask whether we can have a very small $\lambda$ and have the Robust PCA framework hold. In particular, we specialize to the rank 1 case, and it turns out that it cannot be done, as demonstrated in the following.

Recall that if we directly apply Robust PCA we will get

\begin{align*}
 \min_{M=L+S,rank(L)\le1}\|L\|_{*}+\lambda\|S\|_{1}
 & = \min_{S=M-pq^{T},L=pq^{T}}\|L\|_{*}+\lambda\|S\|_{1} \\
 & = \min_{S=M-pq^{T},L=pq^{T}}\|pq^{T}\|_{*}+\lambda\|M-pq^{T}\|_{1} \\
 & = \min_{p,q:\|p\|_{2}=1}\|pq^{T}\|_{*}+\lambda\|M-pq^{T}\|_{1} \\
 & = \min_{p,q:\|p\|_{2}=1}\|q\|_{2}+\lambda\|M-pq^{T}\|_{1} \\
 & = \min_{p:\|p\|_{2}=1}\min_{q}\|q\|_{2}+\lambda\|M-pq^{T}\|_{1}
\end{align*}


Now, for every fixed $p$, consider the subproblem of directly
applying Robust PCA with $\lambda\le\frac{1}{n}$,

\begin{align*}
\min_{q}\|q\|_{2}+\lambda\|M-pq^{T}\|_{1}
& = \min_{q}\max_{\|u\|_{2}\le1,\|V\|_{\infty\le1}}u^{T}q+\lambda Tr(V^{T}(M-pq^{T})) \\
& = \max_{\|u\|_{2}\le1,\|V\|_{\infty\le1}}\min_{q}u^{T}q+\lambda Tr(V^{T}(M-pq^{T})) \\
& = \max_{\|u\|_{2}\le1,\|V\|_{\infty\le1},u=\lambda V^{T}p} \lambda Tr(V^{T}M) \\
& = \max_{\|\lambda V^{T}p\|_{2}\le1,\|V\|_{\infty\le1}} \lambda Tr(V^{T}M) \\
& = \max_{\|V^{T}p\|_{2}\le n,\|V\|_{\infty\le1}} \lambda Tr(V^{T}M)
\end{align*}


Now note that, since $\|p\|_{2}\le1$, we have $\|V^{T}p\|_{2}\le\sqrt{\sum_{i=1}^{n}\sigma_{i}(V^{T}V)}=\sqrt{Tr(V^{T}V)}\le\sqrt{n^{2}\|V\|_{\infty}}$.
Thus, the optimal value is
\begin{align*}
\min_{p,q:\|p\|_{2}=1}\|pq^{T}\|_{*}+\lambda\|M-pq^{T}\|_{1}
& = \min_{p:\|p\|_{2}=1}\max_{\|V\|_{\infty\le1}}\lambda Tr(V^{T}M)\\
& = \lambda \min_{p:\|p\|_{2}=1}\|M\|_{1}\\
& = \lambda \|M\|_{1}
\end{align*}


And it is achieved by $pq^{T}=0$ matrix, which deviates from what
we expect to recover.


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Simulation }


%------------------------------------------------------------------------------
\subsubsection{Comparison between Robust PCA and $L_{1}$ heuristics}

We perform a numerical comparison of the Robust PCA framework and the $\ell_{1}$ heuristic based on the power iteration method. Note that in using the power iteration method for Robust PCA, we would not update $\mu$ if the value of that iteration is 0 because this will make the algorithm to converge to the wrong value (as observed from simulation, this happens quite frequently so this conditioning is needed).

In the simulation, we randomly generated the entries of $p$ and $q$ as $N(0,1)$ iid. We randomly generate sparse matrix with sparse support uniformly distributed across the $n \times n$ matrix. Each sparse entry is randomly distributed $N(0,1)$. We then plot the graph of different degree of sparsity and the corresponding effectiveness of the optimization heuristic in extracting the original $pq^{T}$. We run the experiment 10 times and show the average. A recovery is consider to be correct if the relative error of the low rank matrix and the recovered low rank matrix is less than $10^{-3}$. We remark that we only use 20 number of iteration in the power iteration method where one iteration means updating all the entries of p and q once (i.e. outer iteration). The result is as follows.

\begin{figure}[h!]
\label{fig:comparison}
\centering
\includegraphics[width=\textwidth]{../figures/compare.jpg}
\caption{Ccomparison of Robust PCA and the $\ell_1$ heuristic in solving rank-1 matrix recovery}
\end{figure}

\subsubsection{$\ell_{1}$ heuristic in higher dimension}
We also try the $\ell_1$ heuristic for cases where~$L$ is known to be of rank higher than one. In particular, we simulate~$L$ to be of rank 2 and try to recover it using the $\ell_1$ heuristic based on the power iteration method. The results show that we do have a good performance when the noise is sparse enough. However, it takes a larger number of iterations to produce accurate results, in particular, we use 200 steps iteration when compared to 20 iteration in the case for rank 1 simulation, to obtain the a comparable accuracy. Intuitively, this result makes sense because the more variables we have in the power iteration method, the more likely it is that each sub-optimization will deviate from the global optimal. And it should then take more steps for it to converge back to the global optimal if it has deviated.
%
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{../figures/rank2case.jpg}
\caption{Performance of $\ell_1$ heuristic in recovering a rank 2 matrix using power iteration}
\label{fig:rank2}
\end{figure}

Finally, in Figure~\ref{fig:rank2s} we also compare the results to those of the Robust PCA framework
%
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{../figures/comparisonwithrank2.jpg}
\caption{Comparison of Robust PCA and the $\ell_1$ heuristic in solving rank 2 matrix recovery}
\label{fig:rank2s}
\end{figure}


