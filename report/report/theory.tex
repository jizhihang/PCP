% !TEX root = report.tex

%============================================================================================
\section{Introduction}
Given an observed matrix $M \in \Rbb^{n_1 \times n_2}$ that is formed as a superposition of a low-rank matrix $L_0$ and a sparse matrix $S_0$,
\[
M = L_0 + S_0
\]
Robust Principal Component Analysis \cite{Candes:2011fk} is the problem of recovering the low-rank and sparse components. Under suitable assumptions on the rank and incoherence of $L_0$, and the distribution of the support of $S_0$, the components can be recovered exactly with high probability, by solving the Principal Component Pursuit (PCP) problem given by

\begin{equation}
\begin{aligned}
&\text{minimize} && \|L\|_* + \lambda \|S\|_1 \\
&\text{subject to} && L+S = M
\label{eq:pcp}
\end{aligned}
\end{equation}

Principal component pursuit minimizes a linear combination of the nuclear norm of a matrix $L$ and the $\ell_1$ norm of $M-L$. Minimizing the $\ell_1$ norm is known to favor sparsity, while minimizing the nuclear norm $\|L\|_* = \sum_{\sigma \in \sigma(L)} \sigma$ is known to favour low-rank matrices (intuitively, favors sparsity of the vector of singular values).

The low-rank component $S_0$ is viewed as a noise matrix, that can represent measurement noise, failure in some sensors that will result in completely corrupting a fraction of the observed entries, or missing data (which translates to having a fraction of the entries equal to zero). In this setting, one would like to be able to recover the original data $L_0$, without making assumptions on the magnitude $\|S_0\|_\infty$ of the sparse component, where $\|S\|_\infty = \max_{i,j} |S_{ij}|$. PCP achieves recovery with high probability in this setting, under alternate assumptions on the structure of $L_0$ and sparsity pattern of $S_0$.

One cannot expect to recover the components exactly in the most general case. Assume for example that $L_0$ is such that $(L_0)_{ij} = \delta_i^1\delta_j^1$, and $S_0 = -L_0$. Both matrices are sparse and low-rank, and clearly one cannot expect to recover the components in this case, since the observed matrix is $M = 0$. Therefore assumptions need to be made on the incoherence of $L_0$ and the support of $S_0$.


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Incoherence of the low rank component $L_0$}
The Incoherence conditions describe how much the singular vectors of a given matrix are aligned with the vectors of the canonical basis.

Let the SVD of $L_0$ be given by

\begin{align}
L_0 = U\Sigma V^* = \sum_{i=1}^r \sigma_i u_i v_i^*
\label{eq:svd}
\end{align}

where $U \in \Rbb^{n_1 \times r}$ and $V \in \Rbb^{n_2 \times r}$ are the matrices of left and right singular vectors respectivley, $U = [u_1, \dots, u_r]$, $V = [v_1, \dots, v_r]$. Then the incoherence conditions are given by

\begin{equation}
\begin{aligned}
\max_i \|U^*e_i\|_2^2 \leq \frac{\mu r}{n_1}, && \max_i \|V^*e_i\|_2^2 \leq \frac{\mu r}{n_2}
\label{eq:incoherence1}
\end{aligned}
\end{equation}

and

\begin{equation}
\| U V^* \|_\infty \leq \sqrt{\frac{\mu r}{n_1 n_2}}
\label{eq:incoherence2}
\end{equation}

Note that the condition $\|U^*e_i\|_2^2 \leq \frac{\mu r}{n_1}$ translates to $\sum_{k = 1}^r (u_k)_i^2 \leq \frac{\mu r}{n_1}$.  Also note that the orthogonal projection $P_U$ on $\text{Span}(u_1, \dots, u_r)$ is given by
\[
UU^* = [u_1, \dots, u_r]
\left[ \begin{array}{c}
u_1^* \\
\vdots \\
u_r^* \\
\end{array} \right]
= \sum_{k = 1}^r u_ku_k^*
\]
and the condition is equivalent to $\|P_U e_i \|_2^2 \leq \frac{\mu r}{n_1}$ since $\|U^* e_i \|_2^2 = e_i^*(UU^*)e_i = e_i^* P_U e_i = (e_i - P_Ue_i + P_U e_i )^*P_U e_i = \|P_U e_i\|_2^2$ ($P_U e_i$ and $e_i - P_U e_i$ are orthogonal). Or simply $\|P_U e_i\|_2^2 = e_i^* UU^*UU^*e_i = e_i^* UU^*e_i = \|U^*e_i\|_2^2$ since $U^*U = I_r$.

These conditions require the singular vectors to be ``spread'' enough with respect to the canonical basis. Intuitively, if the singular vectors of the low-rank matrix $L_0$ are aligned with a few canonical basis vectors, then $L_0$ will be sparse and hard to distinguish from the sparse corruption matrix $S_0$.



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Support of the sparse component $S_0$}
The cardinality of the support of $S_0$ is denoted $m$. Guaranteeing exact recovery requires $m$ to be small enough, in a sense that will be defined in the next section. Proving exact recovery will rely on a probabilistic argument which assumes the sparse matrix $S_0$ is drawn from a uniform distribution on the set of matrices with support of a fixed cardinality $m$, i.e. $\{ S \in \Rbb^{n_1 \times n_2} | \mathbf{card}(\mathbf{supp}(S)) = m \}$. Here $\mathbf{card}$ denotes the cardinality, and $\mathbf{supp}$ denotes the set of non zero elements, or support. The proof of the main result will use a different sampling model, and prove equivalence with the uniform model.


%============================================================================================
\section{Main Result}
\begin{theorem}
\label{thm:pcp}
Suppose $L_0 \in \mathbb{R}^{n \times n}$ satisfies incoherence conditions~(\ref{eq:incoherence1}) and (\ref{eq:incoherence2}) and that the support of $S_0$ is uniformly distributed among all sets of cardinality $m$. Then $\exists c$ such that with high probability over the choice of support of $S_0$ (at least $1-cn^{-10}$), Principal Component Pursuit with $m = 1/\sqrt{n}$ is exact, i.e. $\hat{L} = L_0$ and $hat{S} = S_0$ provided that

\begin{equation}
\begin{aligned}
\text{rank}(L_0) \leq \frac{\rho_r}{\mu} \frac{n}{(\log n)^2} && \text{ and } && m \leq \rho_s n^2
\end{aligned}
\end{equation}

\end{theorem}

Above, $\rho_r$ and $\rho_s$ are positive numerical constants. Note in particular that no assumptions are made on the magnitudes of the nonzero entries of $S_0$.

The first condition in the theorem bounds the rank of $L_0$, but also how spread the singular vectors have to be, since we need to have $\forall i$ (from the incoherence condition)
\[
\|U^*e_i\|_2^2 \leq \frac{\mu \text{rank}(L_0)}{n} \leq \frac{\rho_r}{(\log n)^2}
\]

The second condition bounds the size $m$ of the support of $S_0$.
%============================================================================================
\section{Proof}

The main arguments of the proof are the following:

First, change the model of the sparse matrix $S_0$ from the uniform sampling model, to the Bernoulli sampling model with fixed signs, then to the Bernoulli sampling model with random signs. To show equivalence of the results under the different sampling models, an elimination theorem is used.

Then using the random sign Bernoulli sampling model, it is shown that a dual certificate can be constructed with high probability, proving that $(L_0, S_0)$ is the unique optimizer, by constructing a subgradient that shows that any non-zero perturbation $H$ will result in a strict increase in the objective value $\|L_0 + H\|_* + \lambda \|S_0 - H\|_1$.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Preliminaries}
\begin{itemize}
\item The subgradient of of the $\ell_1$ norm at $S_0$ supported on $\Omega$ is of the form $\sgn(S_0)+F$ where $P_\Omega F = 0$ and $\|F\|_\infty \leq 1$.
\item The subgradient of the nuclear norm (for details see Section~\ref{sub:nuclearnorm}) at $L_0 = U\Sigma V^*$ where $U, V \in \Rbb^{n \times r}$ and $\Sigma \in \Rbb^{r \times r}$, is of the form $U V^* + W$, where
\begin{equation}
\label{eq:nuc_norm_subg_cond_1}
\begin{aligned}
&U^*W = 0 \\
& WV = 0\\
& \|W\| \leq 1
\end{aligned}
\end{equation}

where $\|W\|$ denotes the operator norm of matrix $W$, i.e. $\|W\| = \underset{\|u\|_2 = 1}{\max} \|Wu\|_2 = \sigma_{\max}(W)$. Conditions~(\ref{eq:nuc_norm_subg_cond_1}) are equivalent to

\begin{equation}
\begin{aligned}
& P_T W = 0 \\
& \|W\| \leq 1
\end{aligned}
\end{equation}

where $T$ is the linear space of matrices defined by
\begin{equation}
\label{eq:T_subspace}
T = \{UX^* + YV^*, \  X, Y \in \mathbb{R}^{n \times r}\}
\end{equation}

Indeed, we have
\begin{align*}
P_TW = 0
&\Leftrightarrow W \in T^\perp  \\
&\Leftrightarrow \forall M \in T, \ Tr(W^*M) = 0\\
&\Leftrightarrow \forall X, Y \in \mathbb{R}^{n \times r}, \ Tr(W^*(UX^* + YV^*)) = 0\\
&\Leftrightarrow \forall X, Y \in \mathbb{R}^{n \times r}, \ Tr((U^*W)^*X^*) + Tr((WV)^*Y) = 0\\
&\Leftrightarrow U^*W = WV^* = 0
\end{align*}


\end{itemize}

Note that the projection on the orthogonal of $T$ is given by
\begin{equation}
P_{T^\perp} M = (I - UU^*)M(I - VV^*) \label{property: p1}
\end{equation}

\begin{proof}
Note that $UU^*$ is the orthogonal projection on the subspace spanned by the columns of $U$, and similarly for $VV^*$. Let $P_U = UU^*$, $P_{U^\perp} = I - UU^*$, and similarly for $V$.

Let $M_1 = (I - UU^*)M(I - VV^*) = P_{U^\perp} M P_{V^\perp}$. We have
\[
P_{T^\perp} M = M_1 \Leftrightarrow (M - M_1 \in T \text{ and } M_1 \perp M - M_1)
\]
and we have $M - M_1 = UU^*M + MVV^* - UU^*MVV^* = U(U^*M) + (MV - UU^*MV)V^* \in T$, and
\begin{align*}
Tr(M_1^* (M - M_1))
&= Tr((I-VV^*)M^*(I-UU^*)(UU^*M + MVV^* - UU^*MVV^*)) \\
&= Tr(P_{V^\perp}M^*P_{U^\perp}(P_UM + (M - UU^*M)P_V)) \\
&= Tr(P_{V^\perp}M^*P_{U^\perp}P_UM) + Tr((M - UU^*M)P_VP_{V^\perp}M^*P_{U^\perp})\\
&=0
\end{align*}
using the fact that $P_{U^\perp}P_U = P_V P_{V^\perp} = 0$ (projecting consecutively on a subspace and its orthogonal yields 0, or simply expanding, $(I-UU^*)UU^* = UU^* - UU^*UU^* = UU^* - U I_r U^* = 0$). This completes the proof.
\end{proof}

Note that since $P_{T^\perp}$ is an orthogonal projection, we have
\begin{equation}
\| P_{T^\perp} M \| \leq \|M\| \label{property: p2}
\end{equation}

and for any dyad $e_i e_j^*$, we have

%TODO: say why and where this is used?

\begin{align*}
\|P_{T^\perp} e_ie_j^*\|_F^2
&= Tr\left( (I-UU^*)e_ie_j^*(I-VV^*)(I-VV^*)^*e_je_i^*(I-UU^*)^* \right) \\
&= Tr\left( e_i^*(I-UU^*)^*(I-UU^*)e_ie_j^*(I-VV^*)(I-VV^*)^*e_j \right) \\
&= Tr\left( e_i^*(I-UU^*)^*(I-UU^*)e_i\right) Tr \left(e_j^*(I-VV^*)(I-VV^*)^*e_j \right) \\
&= \|(I - UU^*)e_i\|_2^2 \|(I - VV^*)e_j\|_2^2
\end{align*}

and since $UU^*$ is an orthogonal projection, we have
\begin{align*}
\|(I - UU^*)e_i\|_2^2
&= \|e_i\|_2^2 - \|UU^*e_i\|_2^2 \\
&\geq 1 - \mu r / n
\end{align*}

where the last inequality results form the incoherence condition~(\ref{eq:incoherence2}), $\|U^*e_i\|_2^2 \leq \frac{\mu r}{n}$. Therefore

\begin{equation}
\|P_{T^\perp} e_ie_j^*\|_F^2 \geq (1 - \mu r / n)^2   \label{property: p3}
\end{equation}

equivalently, using the fact that $\|P_{T^\perp} e_ie_j^*\|_F^2 + \|P_{T} e_ie_j^*\|_F^2 = \|e_ie_j^*\|_F^2 = 1$, we have

\begin{align*}
\|P_{T} e_ie_j^*\|_F^2
&\leq 1 - \left(1 - \frac{\mu r}{n} \right)^2 \\
&= \frac{2\mu r}{n} - \left( \frac{\mu r}{n} \right)^2 \\
&\leq \frac{2\mu r}{n}
\end{align*}

Thus
\begin{equation}
  \|P_{T} e_ie_j^*\|_F^2 \leq \frac{2\mu r}{n}   \label{property: p4}
\end{equation}
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Elimination Theorem}
The following elimination theorem states the intuitive fact that if PCP exactly recovers the components of $M = L+S$, then it also exactly recovers the components of $M = L+S'$ where $S'$ is a trimmed version of $S$ ($\text{supp}(S')\subset \text{supp}(S)$ and $S$ and $S'$ coincide on $\text{supp}(S')$)

\begin{theorem}
Suppose the solution to the PCP problem (\ref{eq:pcp}) with input data $M_0 = L_0 + S_0$ is unique and exact, and consider $M_0' = L_0 + S_0'$ where $S_0'$ is a trimmed version of $S_0$. Then the solution to (\ref{eq:pcp}) with input $M_0'$ is exact as well.
\end{theorem}

\begin{proof}
Let $S_0' = P_{\Omega_0} S_0$ and let $(\hat{L}, \hat{S})$ be the solution to (\ref{eq:pcp}) with input $L_0 + S_0'$. Then since $(L_0, S_0')$ is a feasible point for (\ref{eq:pcp}), it provides un upper bound on the optimal value
\[
\|\hat{L}\|_* + \lambda \|\hat{S}\|_1 \leq \|L_0\|_* + \lambda \|S_0'\|_1
\]
then decomposing $S_0$ into the orthogonal components $S_0 = P_{\Omega_0} S_0 + P_{\Omega_0^\perp} S_0 = S_0' + P_{\Omega_0^\perp} S_0$, we have $\|S_0'\|_1 = \|S_0\|_1 - \|P_{\Omega_0^\perp} S_0\|_1$, thus we have

\[
\|\hat{L}\|_* + \lambda \|\hat{S}\|_1 + \lambda \|P_{\Omega_0^\perp} S_0\|_1 \leq \|L_0\|_* + \lambda \|S_0\|_1
\]

and using the triangular inequality

\[
\|\hat{L}\|_* + \lambda \|\hat{S} + P_{\Omega_0^\perp} S_0\|_1 \leq \|L_0\|_* + \lambda \|S_0\|_1
\]

we observe that $(\hat{L}, \hat{S} + P_{\Omega_0^\perp} S_0)$ is feasible for the problem with input $M = L_0 + S_0$, for which the optimal value is precisely $\|L_0\|_* + \lambda \|S_0\|_1$. Therefore by uniqueness of the solution, we have
\[
\begin{aligned}
&\hat{L} = L_0 \\
&\hat{S} + P_{\Omega_0^\perp} S_0 = S_0
\end{aligned}
\]

the second equality is equivalent to $\hat{S} = S_0 - P_{\Omega_0^\perp} S_0 = P_{\Omega_0} S_0 = S_0'$. This completes the proof.
\end{proof}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Derandomization}
Derandomization is used to show equivalence between the problem where the signs of the entries of $S_0$ are random, and the problem where the entries of $S_0$ have fixed signs.

In the setting of Theorem~\ref{thm:pcp}, the non-zero entries of the sparse component $S_0$ are fixed, but the proof will use a stronger assumption: the signs of the non-zero entries are independent Bernoulli variables. The following theorem shows equivalence of the two settings.

\begin{theorem}
Suppose $L_0$ satisfies conditions of Theorem~\ref{thm:pcp}, and that the support of $S_0$ is sampled from a Bernoulli model with parameter $2\rho_s$, and the signs of $S_0$ are i.i.d. Bernoulli $\pm 1$ with parameter $\frac{1}{2}$, and independent from the support. Then:

If the PCP solution is exact with high probability, then it is exact with at least the same probability for the model in which signs of $S_0$ are fixed and the support is sampled from a Bernoulli distribution with parameter $\rho_s$.
\end{theorem}

\begin{proof}
Consider the fixed values model, and let $S_0 = P_\Omega S$ for some matrix $S$, and the support $\Omega$ is sampled from a Bernoulli distribution. Thus the components of $S_0$ are independent and
\[
(S_0)_{ij} =
\begin{cases}
S_{ij} & \text{w.p. } \rho_s \\
0 & \text{w.p. } 1 - \rho_s
\end{cases}
\]

the idea of the proof is to craft a new model, and show that it is equivalent (in terms of probability distribution) to the above model.

Let $E$ be a random sign matrix, with i.i.d. entries
\[
E_{ij} =
\begin{cases}
1 & \text{w.p. } \rho_s \\
0 & \text{w.p. } 1 - 2\rho_s\\
-1 & \text{w.p. } \rho_s
\end{cases}
\]
and $\Delta(E)$ an elimination matrix, function of $E$, defined as
\[
\Delta_{ij} =
\begin{cases}
0 & \text{if } E_{ij}S_{ij} < 0 \ (E_{ij} \text{ and } S_{ij} \text{ have different signs} )\\
1 & \text{otherwise}
\end{cases}
\]

the entries of $\Delta$ are functions of independent variables, and are therefore independent.

Now consider the following variable
\[
S'_0 = \Delta \circ |S| \circ E
\]
where $\circ$ is the component wise product. Then $S_0$ and $S'_0$ have the same distribution. Indeed, it suffices by independence to check that they have the same marginals:

\begin{align*}
P((S'_0)_{ij} = S_{ij})
&= P(\Delta_{ij} = 1 \text{ and } E_{ij} = \sgn(S_{ij}) ) \\
&= P(E_{ij}S_{ij} \geq 0 \text{ and } E_{ij} = \sgn(S_{ij})) \\
&= P(E_{ij} = \sgn S_{ij}) \\
&= \rho_s
\end{align*}
and
\[
P(S_0 = S_{ij}) = \rho_s
\]

Finally, since, by assumption, PCP recovers $|S| \circ E$ with high probability, then by the elimination theorem, it also recovers $\Delta \circ |S| \circ E$ with at least the same probability. The result follows since $S'_0$ and $S_0$ have the same distribution.
\end{proof}

We remark that the uniform sampling and the iid Bernoulli sampling model are indeed equivalent and the justification is given in Section~\ref{sub:bernoullisampling}.
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Dual certificate}
The following lemma gives a simple sufficient condition for the pair $(L_0,S_0)$ to be the unique optimal solution to PCP.

\begin{lemma}
Assume that $\|P_\Omega P_T \| < 1$. Then $(L_0,S_0)$ is the unique solution to PCP if $\exists (W, F)$ such that
\begin{equation}
\begin{aligned}
& UV^* + W = \lambda(\sgn(S_0) + F) \\
& P_T W = 0 \\
& \|W\|<1 \\
& P_\Omega F = 0 \\
& \|F\|_\infty < 1
\end{aligned}
\end{equation}

\end{lemma}


\begin{proof}
We first prove that the condition $\|P_\Omega P_T\| < 1$ is equivalent to $\Omega \cap T = \{0\}$.

First, if $\Omega \cap T \neq \{0\}$, then let $M_0 \in \Omega \cap T$, $M_0 \neq 0$. We have $\| P_\Omega P_T M_0\| = \|M_0\|$, thus $\| P_\Omega P_T\| = \underset{M \neq 0}{\max} \frac{\| P_\Omega P_T M\|}{\|M\|} \geq 1$.

Conversely, if $\| P_\Omega P_T\| \geq 1$, then $\exists M_0 \neq 0$ such that $\|M_0\| \leq \| P_\Omega P_T M_0\|$. But since $P_\Omega$ and $P_T$ are orthogonal projections, we have $\|M_0\| \leq \| P_\Omega P_T\| \leq \|P_T M_0\| \leq \|M_0\|$, where inequalities must hold with equality. In particular, we have $\|P_T M_0\| = \|M_0\|$, which implies $P_T M_0 = M_0$ (to prove this, decompose $\|M_0\|$ into the orthogonal components $\|M_0\|^2 = \|M_0 - P_TM_0\|^2 + \|P_TM_0\|^2$, thus $\|P_TM_0\| = \|M_0\| \Rightarrow \|M_0 - P_TM_0\| = 0 \Rightarrow M_0 = P_T M_0$), then similarly, $\|P_\Omega M_0\| = \|M_0\|$, which implies $P_\Omega M_0 = M_0$. Therefore $M_0 \in \Omega \cap T$. This proves the equivalence $\|P_\Omega P_T\| < 1 \Leftrightarrow \Omega \cap T = \{0\}$.



To prove that $(L_0, S_0)$ is the unique optimizer, we show that for any feasible perturbation $(L_0 + H, S_0 - H)$ where $H \neq 0$ strictly increases the objective. Let
\begin{itemize}
\item $UV^* + W_0$ be an arbitrary subgradient of the nuclear norm at $L_0$, where $\|W_0\| \leq 1$ and $P_T W_0 = 0$
\item $\sgn (S_0) + F_0$ be an arbitrary subgradient of the $\ell_1$-norm at $S_0$, where $\|F_0\|_\infty \leq 1$ and $P_\Omega F_0 = 0$
\end{itemize}

Then we can lower bound the value of the objective
\[
\|L_0 + H\|_* + \lambda \|S_0 - H\|_1 \geq \|L_0\|_* + \lambda \|S_0\|_1 + \langle UV^* + W_0, H\rangle - \lambda \langle \sgn(S_0) + F_0, H\rangle
\]

Now we pick a particular pair $(W_0, F_0)$ such that
\begin{itemize}
\item $\langle W_0, H \rangle = \|P_{T^\perp} H\|_*$, for example $W_0 = P_{T^\perp} W$ where $W$ is a normed matrix such that $\langle W, P_{T^\perp} H \rangle = \|P_{T^\perp} H\|_*$ (by duality of $\|.\|$ and $\|.\|_*$)
\item $\langle F_0, H \rangle = -\|P_{\Omega^\perp} H\|_1$, for example $F_0 = -\sgn(P_{\Omega^\perp} H)$
\end{itemize}
then we have

\[
\|L_0 + H\|_* + \lambda \|S_0 - H\|_1 \geq \|L_0\|_* + \lambda \|S_0\|_1  + \|P_{T^\perp} H\|_* + \|P_{\Omega^\perp} H\|_1 + \langle UV^* - \lambda  \sgn(S_0), H\rangle
\]

we can bound the inner product using the definition of $W$ and $F$,

\begin{align*}
|\langle UV^* - \lambda  \sgn(S_0), H\rangle|
& = |\langle \lambda F - W, H\rangle|  & \text{since } UV^* + W = \lambda(\sgn(S_0) + F) \\
&\leq |\langle W, H \rangle| + \lambda |\langle F, H \rangle| & \text{by the triangular inequality} \\
&\leq \beta (\|P_{T^\perp} H\|_* + \lambda \|P_{\Omega^\perp} H\|_1)
\end{align*}
where $\beta = \max (\|W\|, \|F\|_\infty) < 1$, and the last inequality follows from the fact that
\[
\begin{aligned}
& \|P_{T^\perp} H\|_* \geq \langle P_{T^\perp} H, W/\|W\| \rangle \geq \langle H, W/\|W\| \rangle\\
& \|P_{\Omega^\perp} H\|_1 \geq \langle P_{\Omega^\perp} H, F/\|F\|_\infty \rangle \geq \langle H, F/\|F\|_\infty \rangle
\end{aligned}
\]
Thus
\begin{align*}
\|L_0 + H\|_* + \lambda \|S_0 - H\|_1 - \|L_0\|_* - \lambda \|S_0 \|_1
&\geq  (1-\beta) \left( \|P_{T^\perp} H\|_* + \lambda \|P_{\Omega^\perp} H\|_1 \right) \\
& > 0
\end{align*}
since $\|P_{T^\perp} H\|_* = \|P_{\Omega^\perp} H\|_1 = 0$ only if $P_{T^\perp} H = P_{\Omega^\perp} H = 0$, i.e. $H \in \Omega \cap T$, and, by assumption, $\Omega \cap T = {0}$ and $H \neq 0$. Therefore the objective strictly increases with a non-zero perturbation. This completes the proof.

\end{proof}


The proof of the main theorem will use a slightly different result, given by the following Lemma
\begin{lemma}
\label{lem:dual_cert}
Assume that $\|P_\Omega P_T \| \leq 1/2$. Then $(L_0,S_0)$ is the unique solution to PCP if $\exists (W, F)$ such that

\begin{equation}
\label{eq:dual_cert_cond}
\begin{aligned}
& UV^* + W = \lambda(\text{sign}(S_0) + F + P_\Omega D) \\
& P_T W = 0 \\
& \|W\| \leq 1/2 \\
& P_\Omega F = 0 \\
& \|F\|_\infty \leq 1/2 \\
&\|P_\Omega D \|_F \leq 1/4
\end{aligned}
\end{equation}

\end{lemma}

\begin{proof}
Using $\beta = \max (\|W\|, \|F\|_\infty) \leq \frac{1}{2}$ in the previous proof, we have for a non-zero perturbation $H$
\begin{align*}
\|L_0 + H\|_* + \lambda \|S_0 - H\|_1 - \|L_0\|_* - \lambda \|S_0 \|_1
&\geq  \frac{1}{2} \left( \|P_{T^\perp} H\|_* + \lambda \|P_{\Omega^\perp} H\|_1 \right) - \lambda \langle P_{\Omega} D, H\rangle \\
& \geq \frac{1}{2} \left( \|P_{T^\perp} H\|_* + \lambda \|P_{\Omega^\perp} H\|_1 \right) - \frac{\lambda}{4} \|P_{\Omega} H\|_F
\end{align*}

the last term can be further bounded
\begin{align*}
\|P_\Omega H\|_F
&\leq \|P_\Omega P_T H\|_F + \|P_\Omega P_{T^\perp} H\|_F \\
&\leq \frac{1}{2} \|H\|_F + \|P_{T^\perp} H\|_F & \text{using } \|P_\Omega P_T\| \leq \frac{1}{2} \text{ and } \|P_\Omega\| \leq 1 \\
&\leq \frac{1}{2} \|P_\Omega H\|_F + \frac{1}{2} \|P_{\Omega^\perp} H\|_F + \|P_{T^\perp} H\|_F
\end{align*}

therefore

\[
\|P_\Omega H\|_F \leq \|P_{\Omega^\perp} H\|_F + 2 \|P_{T^\perp} H\|_F
\]

and we conclude by lower bounding the increase in the objective
\begin{align*}
\|L_0 + H\|_* + \lambda \|S_0 - H\|_1 - \|L_0\|_* - \lambda \|S_0 \|_1
&\geq  \frac{1}{2} \left( (1- \lambda) \|P_{T^\perp} H\|_* + \frac{\lambda}{2} \|P_{\Omega^\perp} H\|_1 \right) \\
& > 0
\end{align*}
since $ \|P_{T^\perp} H\|_* =  \|P_{\Omega^\perp} H\|_1 = 0$ only if $P_{\Omega^\perp} H = P_{T^\perp} H = 0$, i.e. $H \in \Omega \cap T$, and, by assumption, $\Omega \cap T = \{ 0 \}$ ($\|P_\Omega P_T\| \leq \frac{1}{2} < 1$).
This completes the proof.

\end{proof}

\iffalse
By the previous Lemma, it suffices to produce a dual certificate $W$ such that
\begin{equation}
\begin{aligned}
& W \in T^{\perp} \\
& \|W\| < 1/2\\
& \|P_\Omega (UV^* - \lambda \sgn(S_0) + W) \|_F \leq \lambda / 4  \\
& \|P_{\Omega^\perp}(UV^* + W)\|_\infty < \lambda / 2
\end{aligned}
\end{equation}

since under these conditions, $D = \frac{1}{\lambda} P_\Omega (UV^* - \lambda \sgn(S_0) + W)$ and $F = \frac{1}{\lambda} P_{\Omega^\perp} (UV^* + W)$ satisfy the sufficient conditions given by Lemma~\ref{lem:dual_cert}. Indeed we have

\begin{align*}
UV^* + W - \lambda \sign(S_0)
&= P_\Omega (UV^* + W - \lambda \sign(S_0))+ P_{\Omega^\perp} (UV^* + W - \lambda \sign(S_0)) \\
&= \lambda D+ P_{\Omega^\perp} (UV^* + W) \text{ since } \sign(S_0) \in \Omega\\
&= \lambda (D+F)
\end{align*}

and the first condition of Lemma~\ref{lem:dual_cert} is satisfied. The remaining conditions follow from the definition of $F$ and $D$.
\fi

%-----------------------------------------------------------------------------
\subsubsection{Bounding $\|P_\Omega P_T\|$}
 Under suitable conditions on the size of the support $\Omega_0$ of the sparse component, a bound can be derived on  $\|P_\Omega P_T\|$ \cite{Candes:2009uq}.
\begin{theorem}
Suppose $\Omega_0$ is sampled from the Bernoulli model with parameter $\rho_0$. Then with high probability,
\[
\|P_T - \rho_0^{-1} P_T P_{\Omega_0} P_T\| \leq \epsilon
\]
provided that $\rho_0 \geq C_0 \epsilon^{-2} \frac{\mu r \log n}{n}$ where $\mu$ is the incoherence parameter and $C_0$ is a numerical constant.
\end{theorem}

As a consequence, $\|P_\Omega P_T\|$ can be bounded, and if $|\Omega|$ is not too large, then the desired bound $\|P_\Omega P_T\| \leq 1/2$ holds.

\subsection{Probabilistic Guarantee via Dual Certification}

We now present the proof that, under the assumptions of Robust PCA, then with high probability, we can find a dual certificate $(W,F,D)$ that satisfies the conditions~(\ref{eq:dual_cert_cond}) of Lemma~\ref{lem:dual_cert}. This will achieve the proof of the probabilistic guarantee of recovery of $(L_0, S_0)$.

In order to construct a dual certificate $(W, F, D)$, we define $W = W^{L}+W^{S}$ where $W^{L}$ and $W^{S}$ are constructed to satisfy the following
properties. First, $P_{\Omega}(W^{S}) = \lambda \sign(S_0)$. Then it also need to satisfies the following.

\begin{lemma} \label{wl} Let $S_0\sim Bern(\rho)$ iid for each entry with $\Omega$
as its support set. Set $j_{0}=2\log n$. With the assumptions in
the main theorem of RPCA, $W^{L}$ satisfies the following with high
probability.
\begin{enumerate}
\item $||W^{L}||<\frac{1}{4}$
\item $||P_{\Omega}(UV^{*}+W^{L})||_{F}<\frac{\lambda}{4}$
\item $||P_{\Omega^{\bot}}(UV^{*}+W^{L})||_{\infty}<\frac{\lambda}{4}$
\end{enumerate}
\end{lemma}

\begin{lemma} \label{ws} Let $S_0\sim Bern(\rho)$ iid for each entry with $\Omega$
as its support set. With the assumptions in the main theorem of RPCA,
$W^{S}$ satisfies the following with high probability.
\begin{enumerate}
\item $||W^{S}||<\frac{1}{4}$
\item $||P_{\Omega^{\bot}}(W^{S})||_{\infty}<\frac{\lambda}{4}$
\end{enumerate}
\end{lemma}

After having these two lemmas, we are ready to show that they help to justify the probabilistic guarantee. We note that
\begin{eqnarray*}
UV^{*}+W & = & W^{L}+W^{S}+UV^{*}\\
 & = & \lambda \left[ P_{\Omega} \left( \frac{UV^{*}+W^{L}}{\lambda} \right) + sign(S_{0}) + P_{\Omega^{\bot}} \left( \frac{W^{L}+W^{S}+UV^{*}}{\lambda} \right) \right]
\end{eqnarray*}
Thus we take
\begin{eqnarray*}
D & = & \frac{UV^{*}+W^{L}}{\lambda}\\
W & = & W^{L}+W^{S}\\
F & = & P_{\Omega} \left( \frac{W^{L}+W^{S}+UV^{*}}{\lambda} \right)
\end{eqnarray*}


From Lemma~(\ref{wl}) and Lemma~(\ref{ws}), we can check that $(W,F,D)$ satisfies the conditions of Lemma~(\ref{lem:dual_cert}), thus establishing the probabilistic guarantee.

The following section will discuss in depth how to construct $W^{L}$ and $W^{S}$ that satisfy conditions of Lemma~(\ref{wl}) and Lemma~(\ref{ws}). 