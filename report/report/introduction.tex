% !TEX root = report.tex
%============================================================================================
\section*{Introduction}

Over the past decade there has been an explosion in terms of the massive amounts of high-dimensional data in almost all fields of science and engineering. This situation presents a challenge as well as an opportunity to many areas such as web data analysis, search, biomedical imaging, bioinformatics, (social) network analysis, image, video and multimedia processing and many others. In such applications, researches today routinely deal with data that lie in thousands or even billions of dimensions, with a number of samples sometimes of the same order of magnitude.

Often, in order to be able to even make sense of data in such high dimensionality and scale, one has to leverage on the fact that such data often have low intrinsic dimensionality. One reasonable (but not the only) approach that is well justified in many practical situations is to assume that the data all lie near some low-dimensional subspace. In mathematical terms, this amounts to saying that if all the data points are stacked as column vectors of a matrix $M$, the matrix should (approximately) have low rank. Specifically, a common model is to assume that 
%
\begin{align}
M=L_0+N_0
\label{IntroMot:lowrankness}
\end{align}
%
where~$L_0$ is of low rank and~$N_0$ is a small perturbation matrix (i.e. the noise). A classical approach to the problem is to seek the best (in an~$\ell_2$-sense) rank-$k$ estimate of~$L_0$ by solving the optimization problem
%
\begin{align}
\min_L \norm{M-L}{2}{} \quad:\quad \rank(L)\leq k
\label{IntroMot:ClassicPCA}
\end{align}
%
This approach, known as Principal Component Analysis (PCA)~\cite{Eckart:1936uq,Hotelling:1933fk}, has been studied for almost a century and is used extensively for data analysis and dimensionality reduction in many areas. It is easy to show that this problem can be efficiently solved using the singular value decomposition (SVD) of the data matrix~$M$. It also enjoys a number of optimality properties when the noise~$N_0$ is small and i.i.d. Gaussian. However, PCA in general may fail spectacularly when the assumption on the noise matrix is not satisfied: Even a single grossly corrupted entry in~$M$ could render the estimated~$\Lhat$ arbitrarily far from the true~$L_0$. Unfortunately, these kinds of gross errors are very common in many applications, arising for example from corrupted data, sensor failures or corrupted samples in repetitive measurement tasks in biology applications. 

We would therefore like a method that is able to extract the principal components (the low rank structure) of measurement data even in the presence of such gross but sparse corruptions. The recently proposed Robust PCA framework~\cite{Candes:2011fk} is a very promising candidate for this task. Robust PCA combines the two popular heuristics of nuclear norm minimization (used to encourage low-rankness) and $\ell_1$-norm minimization (used to encourage sparsity) and casts the problem of separating a low-rank ``data'' component from a sparse ``noise'' component into a convex optimization problem. The surprising fact is that one can show that, under certain reasonable conditions, the recovered solution is exact. 

The objective of this project is threefold: First, we survey the recent results on Robust PCA and related extensions and use the theory of convex optimization to develop an understanding of how the main theoretical results are proven. Second, we discuss some efficient algorithms for Robust PCA that allow the application of the framework to large-scale problems. Finally, we apply the Robust PCA framework to a number of different problems in order to illustrate its potential practical relevance. 