\documentclass{../common/projectreport}
\input{../common/defs.tex}

%============================================================================================

\title{\Large Robust Principal Component Analysis}
\author{Maximilian Balandat \and Walid Krichene \and Chi Pang Lam \and Ka Kit Lam}

%============================================================================================
\begin{document}
\maketitle

%============================================================================================
\section{Introduction}
Given an observed matrix $M \in \mathbb{R}^{n_1 \times n_2}$ that is formed as a superposition of a low-rank matrix $L_0$ and a sparse matrix $S_0$, 
\[
M = L_0 + S_0
\]
Robust Principal Component Analysis \cite{Candes:2011fk} is the problem of recovering the low-rank and sparse components. Under suitable assumptions on the rank and incoherence of $L_0$, and the distribution of the support of $S_0$, the components can be recovered exactly with high probability, by solving the Principal Component Pursuit (PCP) problem given by

\begin{equation}
\begin{aligned}
&\text{minimize} && \|L\|_* + \lambda \|S\|_1 \\
&\text{subject to} && L+S = M
\label{PCP}
\end{aligned}
\end{equation}

Principal component pursuit minimizes a linear combination of the nuclear norm of a matrix $L$ and the $\ell_1$ norm of $M-L$. Minimizing the $\ell_1$ norm is known to favour sparsity, while minimizing the nuclear norm $\|L\|_* = \sum_{\sigma \in \sigma(L)} \sigma$ is known to favour low-rank matrices (intuitively, favours sparsity of the vector of singular values).

One cannot expect to recover the components exactly in the most general case. Assume for example that $L_0$ is such that $(L_0)_{ij} = \delta_i^1\delta_j^1$, and $S_0 = -L_0$. Both matrices are sparse and low-rank, and clearly one cannot expect to recover the components in this case, since the observed matrix is $M = 0$. Therefore assumptions are made on the incoherence of $L_0$ and the support of $S_0$.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Incoherence of the low rank component $L_0$}
The Incoherence conditions describe how much the singular vectors of a given matrix are aligned with the vectors of the canonical basis.

Let the SVD of $L_0$ be given by

\begin{align}
L_0 = U\Sigma V^* = \sum_{i=1}^r \sigma_i u_i v_i^*
\end{align}

Then the incoherence conditions are given by

\begin{equation}
\begin{aligned}
\max_i \|U^*e_i\|_2^2 \leq \frac{\mu r}{n_1}, && \max_i \|V^*e_i\|_2^2 \leq \frac{\mu r}{n_2}
\label{incoherence1}
\end{aligned}
\end{equation}

and

\begin{equation}
\| U V^* \|_\infty \leq \sqrt{\frac{\mu r}{n_1 n_2}} \label{incoherence2}
\end{equation}

These conditions require the singular vectors to be ``spread'' enough with respect to the canonical basis. Intuitively, if the singular vectors of the low-rank matrix $L_0$ are aligned with a few canonical basis vectors, then $L_0$ will be sparse and hard to distinguish from the sparse corruption matrix $S_0$.


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Support of the sparse component $S_0$}

%============================================================================================

\input{theory.tex}

%----------------------------------------------------------------------------------------------------------------------------------------------------------------

\input{theory_proofs.tex}

%============================================================================================
\newpage
\section{Related Problems and Extensions}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Exact Matrix completion}
Robust PCA is an extension of the exact matrix completion problem introduced in \cite{Candes:2009uq}, where one seeks to recover a low-rank matrix $L_0$ from a small fraction of its entries. More precisely, assume one is given $\{(L_0)_{ij}, (i,j)\in \Omega\}$ where $\Omega$ is a subset of $[n]\times [n]$.

Problem to solve

\begin{equation}
\begin{aligned}
&\text{minimize} && \text{rank}(L) \\
&\text{subject to} && P_\Omega L = P_\Omega L_0
\end{aligned}
\end{equation}


A heuristic is to minimize the nuclear norm of $L$

%-------------------------------------------------------------------------
\subsubsection{Incoherence}

Singular vectors have to be sufficiently spread

\begin{equation}
\mu(U) = \frac{n}{r} \max_i \|P_U e_i\|_2^2 = \frac{n}{r} \max_i \left[ \sum_{k=1}^r u_{ki}^2 \right]
\label{emc_incoherence}
\end{equation}

Assumptions:
\begin{itemize}
\item $\max \{\mu(U), \mu(V)\} \leq \mu_0$
\item $\left( \sum_k u_kv_k^*\right)_{ij} \leq \mu_1 \sqrt{\frac{r}{n_1 n_2}}$ (true for $\mu_1 = \mu_0\sqrt{r}$)
\item $m \geq c \max \{ \mu_1^*, \sqrt{\mu_0}\mu_1, \mu_0 n^{1/4}\}n r \beta \log n$
\end{itemize}

Under these assumptions, recovery is exact with high probability (at least $1-\frac{c}{n\beta}$)

Incoherent matrices:
\begin{itemize}
\item sampled from the incoherent basis model
\item sampled from the random orthogonal model: if $M = \sum_k \sigma_k u_k v_k^*$, then $\{u_1, \dots, u_r\}$ and $\{v_1, \dots, v_r\}$ are assumed to be selected at random.

\end{itemize}

%-------------------------------------------------------------------------
\subsubsection{Main result}




%-------------------------------------------------------------------------
\subsubsection{Comparing results to Robust PCA}
Robust PCA can be thought of as an extension of the matrix completion problem, where instead of having a known subset of the entries $\{(L_0)_{ij}, (i,j)\in \Omega\}$ and the rest is missing, we have an unknown subset of the entries and the rest is corrupted. In this sense, Robust PCA is a harder problem.

Note that the matrix $L_0$ can be recovered by Principal Component Pursuit, solving a different problem:
\begin{equation}
\begin{aligned}
&\text{minimize} && \|L\|_* + \lambda \|S\|_1\\
&\text{subject to} && P_\Omega (L+S) = P_\Omega L_0
\end{aligned}
\end{equation}


%TODO: numerical simulations



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Stable Principal Component Pursuit}
\subsubsection{Overview}

The paper studies the problem of recovering a low-rank matrix (the principal components) from a high- dimensional data matrix despite both small entry-wise noise and gross sparse errors. It proves that the solution to a convex program (a relaxation of classic Robust PCA) gives an estimate of the low-rank matrix that is simultaneously stable to small entry- wise noise and robust to gross sparse errors. The result shows that the proposed convex program recovers the low-rank matrix even though a positive fraction of its entries are arbitrarily corrupted, with an error bound proportional to the noise level. 

%-----------------------------------------------------------------------------
\subsubsection{Main result}

The paper consider a matrix $M\in\mathbb{R}^{n_1\times n_2}$ of the from $M = L_0+S_0+Z_0$, where~$L_0$ is (non-sparse) low rank, $S_0$ is sparse (modeling gross errors) and $Z_0$ is ``small'' (modeling a small noisy perturbation). The assumption on~$Z_0$ is simply that $||Z_0||_F \leq \delta$ for some small known~$\delta$. Hence at least for the theory part of the paper the authors do not assume anything about the distribution of the noise other than it is bounded (however they will gloss over this in their algorithm). 

The convex program to be solved is a slight modification of the standard Robust PCA problem and given by
\begin{align}
\begin{split}
\min_{L,S} \; &||L||_* + \lambda ||S||_1 \\
\text{s.t.} \quad &||M-L-S||_F \leq \delta
\end{split}
\label{mainresult:optproblem}
\end{align}
where $\lambda = 1/\sqrt{n_1}$. Under a standard incoherence assumption on~$L_0$ (which essentially means that $L_0$ should not be sparse) and a uniformity assumption on the sparsity pattern of~$S_0$ (which means that the support of~$S_0$ should not be too concentrated) the main result states that, with high probability in the support of~$S_0$, for any~$Z_0$ with $||Z_0||_F \leq \delta$, the solution $(\hat{L},\hat{S})$ to~\eqref{mainresult:optproblem} satisfies
\begin{align*}
||\hat{L}-L_0||_F^2 + ||\hat{S}-S_0||_F^2 \leq C n_1n_2\delta^2
\end{align*}
where~$C$ is a numerical constant. The above claim essentially states that the recovered low-rank matrix~$\hat{L}$ is stable with respect to non-sparse but small noise acting on all entries of the matrix.

In order to experimentally verify the predicted performance to their formulation, the author provide a comparison with an oracle. This oracle is assumed to provide information about the support of~$S_0$ and the row and column spaces of~$L_0$, which allows the computation of the MMSE estimator which otherwise would be computationally intractable (strictly speaking it of course is not really the MMSE, since it uses additional information from the oracle). Simulation results that show that the RMS error of the solution obtained through~\eqref{mainresult:optproblem} in the non-breakdown regime (that is, for the support of~$S_0$ sufficiently small) is only about twice as large as that of the oracle-based MMSE. This suggests that the proposed algorithm works quite well in practice. 



%-----------------------------------------------------------------------------
\subsubsection{Relations to existing work}

The result of the paper can be seen from two different view points. On the one hand, it can be interpreted from the point of view of standard PCA. In this case, the result states that standard PCA, which can in fact be shown to be statistically optimal w.r.t. i.i.d Gaussian perturbations, can also be made robust with respect to sparse gross corruptions. On the other hand, the result can be interpreted from the point of view of Robust PCA. In this case, it essentially states that the classic Robust PCA solution can itself be made robust with respect to some small but non-sparse noise acting on all entries of the matrix. 

Conceptually, the work presented in the paper is similar to the development of results for ``imperfect'' scenarios in compressive sensing where the measurements are noisy and the signal is not exact sparse. In this body of literature, $l_1$-norm minimization techniques are adapted to recover a vector $x_0 \in \mathbb{R}^n$ from contaminated observations $y=Ax_0+z$, where $A \in \mathbb{R}^{m\times n}$ with $m \ll n$ and z is the noise term. 


%-----------------------------------------------------------------------------
\subsubsection{Algorithm}

For the case of a noise matrix~$Z_0$ whose entries are i.i.d. $\Ncal(0,\sigma^2)$, the paper suggests to use an Accelerated Proximal Gradient (APG) algorithm (see algorithms section for details) for solving~\eqref{mainresult:optproblem}. Note that for~$\delta =0$ the problem reduces to the standard Robust PCA problem with an equality constraint on the matrices. For this case the APG algorithm proposed in~\cite{Lin:2009kx} solves an approximation of the form
\begin{align*}
\min_{L,S} \; &||L||_* + \lambda ||S||_1 + \frac{1}{2\mu} ||M-L-S||_F
\end{align*}
For the Stable PCP problem where~$\delta>0$ the authors advocate using the same algorithm with fixed but carefully chosen parameter~$\mu$ (similar to~\cite{Candes:2010fk}). In particular, they point out\footnote{this based on the strong Bai Yin Theorem~\cite{Bai:1988fk}, which implies that for an $n\times n$ real matrix with entries $\xi_{ij} \sim \Ncal(0,1)$ the it holds that $\limsup_{n\rightarrow \infty} \norm{Z_0}{2}{}/\sqrt{n} = 2$ almost surely}
 that for $Z_0 \in\Rbf^{n\times n}$ with~$(Z_0)_{ij} \sim \Ncal(0,\sigma^2)$ i.i.d. it holds that $n^{-1/2}\norm{Z_0}{2}{} \rightarrow \sqrt{2}\sigma$ almost surely as $n\rightarrow\infty$. They then choose the parameter~$\mu$ such that if $M=Z_0$, i.e. if $L_0=S_0=0$, the minimizer of the above problem is likely to be $\hat{L}=\hat{S}=0$. The claim is that this is the case for~$\mu = \sqrt{2n}\sigma$.
  
It is worth noting that the assumption of a Gaussian noise matrix~$Z_0$ is reasonable but not always satisfied. If it is not, then it is not clear if using the APG algorithm to solve the associated approximate problem is a good idea and different algorithms may be needed. The problem~\eqref{mainresult:optproblem} can be expressed as an SDP and can therefore in principle be solved using general purpose interior point solvers. However, the same scalability issues as in the standard Robust PCA problem will limit prohibit to use these methods for high-dimensional data. The paper~\cite{Aybat:2011vn} focuses on efficient first-order algorithms for solving~\eqref{mainresult:optproblem}.


%-----------------------------------------------------------------------------
\subsubsection{Conclusion and Outlook}

The paper addresses a problem of potentially very high practical relevance. While it is reasonable to assume that in many applications the low-rank component~$L_0$ will only be corrupted by a comparatively small number of gross errors (caused by rare and isolated events), the assumption of perfect measurements for the rest of the data outside the support of~$S_0$ that is made in classic Robust PCA will generally not hold for example due to sensor noise. This paper asserts that if the non-sparse noise component~$Z_0$ is sparse, then with high probability the recovered components are ``close'' to the actual ones. 

For simplicity, the paper models the non-sparse noise simply as an additive perturbation that is bounded in the Frobenius norm. In cases where one has additional information available about this noise, for example its distribution or some bounds on the absolute value of each entry, it might be possible to derive better bounds on the resulting errors. One possible extension could therefore be to look at exploiting structure in the noise. 

One thing the paper claims is that ``at a cost not so much higher than the classical PCA, [the] result is expected to have significant impact on many practical problems''. As mentioned above I do agree that the result has a significant impact on many practical problems. However, the claim concerning the computational complexity is very optimistic. The fastest solver for the special case $\delta =0$ (classic Robust PCA) currently seems to be a alternating directions augmented Lagrangian method. This method requires an SVD at each iteration, and for problems involving large-scale data the number of iterations can be very large. The standard PCP algorithm on the other hand is based on a single SVD, hence it can be computed much faster. 


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Robust Alignment by Sparse and Low-rank Decomposition}
\label{subsec: RASL}

The convex optimization framework for low-rank matrix recovery has been employed successfully. However, in practice, much more data can be viewed as low-rank only after some transformation is applied. The new formulation of this problem as Robust Alignment by Sparse and Low-rank Decomposition (RASL) \cite{Peng:2010}:
\begin{align}
\min_{A, E, \tau}  ||A||_{*} + \lambda||E||_{1} \quad  \text{s.t.} \;  D\circ\tau = A+E
\label{eq:rasl:original}
\end{align}
where $A  \in\mathbb{R}^{m\times n}$ is low-rank matrix, $A\in\mathbb{R}^{m\times n}$ is sparse matrix, $D$ is our measurements, which is the result of $(A+E)$ subjecting to transformation $\tau^{-1}$. Here we assume that the transformation is invertible. 
We define $D\circ\tau$ as: $D\circ\tau = [\;D_{1}\circ\tau_{1} \;|\;D_{2}\circ\tau_{2} \;|\; \dots \;|\; D_{n}\circ\tau_{n}\;]$, which is the measurements $D=[\;D_{1} \;|\;D_{2} \;|\; \dots \;|\; D_{n}\;] $subjects to set of transformations $\tau=[\;\tau_{1} \;|\;\tau_{2} \;|\; \dots \;|\; \tau_{n}\;] \in\mathbb{G}^n$, where $\mathbb{G}$ is a group of certain type of invertible transformations, which could be affine transform, rotation transform, etc.  \\

The main difficulty in solving ~\eqref{eq:rasl:original} is the nonlinearity of constraint $D\circ\tau = A+E$. When the change in $\tau$ is small, we can approximate this constraint by linearizing about the current estimate of $\tau$. Here, we assume that $\mathbb{G}$ is some $p$-parameter group and identify $\tau=[\;\tau_{1} \;|\;\tau_{2} \;|\; \dots \;|\; \tau_{n}\;] \in \mathbb{R}^{p\times n}$ with the parameterizations of all of the transformations. For $\Delta\tau = [\;\Delta\tau_{1} \;|\; \Delta\tau_{2} \;|\; \dots \;|\; \Delta\tau_{n}\;]$, write $D\circ(\tau+\Delta\tau) \approx D\circ\tau + \sum_{i=1}^n J_{i}\Delta\tau_{i}\epsilon_{i}$, where $J_{i} \doteq \frac{\partial}{\partial\zeta}(D_{i}\circ\zeta)|_{\zeta = \tau_{i}}$ is the Jacobian of the $i$-th measurement with respect to the transformation parameters $\tau_{i}$. $\{\epsilon_{i}\}$ denotes the standard basis for $\mathbb{R}^n$. This leads to a convex optimization problem in unknowns $A, E, \Delta\tau$:
\begin{align}
\min_{A, E, \Delta\tau}  ||A||_{*} + \lambda||E||_{1}  \quad \text{s.t.} \;  D\circ\tau + \sum_{i=1}^n J_{i}\Delta\tau\epsilon_{i}\epsilon_{i}^{T}= A+E
\label{eq:rasl:linearized}
\end{align}
It leads to algorithm~\ref{alg:RASL}




\begin{algorithm}[h!]
\label{alg:RASL}
\caption{RASL}
\KwIn{$D = [\; D_{1} \;|\; D_{2} \;|\; \dots \;|\; D_{n}]$, initial transformation $\tau_{1}, \tau_{2}, \dots, \tau_{n}$ in a certain parametric group $\mathbb{G}$, weight $\lambda > 0$.}
\While{not converged}{
\textbf{Step 1:} compute Jacobian matrices w.r.t. transformation: %$J_{i} \leftarrow \frac{\partial}{\partial\zeta}(D_{i}\circ\zeta)|_{\zeta = \tau_{i}}$ \\
\begin{equation}
J_{i} \leftarrow \frac{\partial}{\partial\zeta}(D_{i}\circ\zeta)|_{\zeta = \tau_{i}} \nonumber
\end{equation}
\textbf{Step 2 (inner loop):} solve the linearized convex optimization:
\begin{equation}
(A^{*}, E^{*}, \Delta\tau^{*}) \leftarrow \argmin_{A, E, \Delta\tau}  ||A||_{*} + \lambda||E||_{1}  \quad \text{s.t.} \;  D\circ\tau + \sum_{i=1}^n J_{i}\Delta\tau\epsilon_{i}\epsilon_{i}^{T}= A+E  \nonumber
\end{equation}
\textbf{Step 3:} update the transformation: $\tau \leftarrow \tau + \Delta\tau^{*}$
\newline
}
\KwOut{$A^{*}, E^{*}, \tau^{*}$}
\end{algorithm}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Robust Matrix Decomposition With Sparse Corruptions: D Hsu et. al.}

%------------------------------------------------------------------------------
\subsubsection{Question being addressed}

Under deterministic setting, it studies how much sparsity is allowed for accurate recovery of the sparse-lowrank pairs.


%------------------------------------------------------------------------------
\subsubsection{Main ideas}

Given $M$ as the observed matrix, it analyze the following two optimization problems. With arguments as $(L,S)$,

\begin{eqnarray}
\min_{(L,S)} & ||L||_{*}+\lambda||S||_{1}\nonumber \\
s.t. & ||L+S-M||_{1}\le\epsilon_{1}\\
 & ||L+S-M||_{*}\le\epsilon_{*}\nonumber 
\end{eqnarray}


and 
\begin{eqnarray}
\min_{(L,S)} & ||L||_{*}+\lambda||S||_{1}+\frac{1}{2\mu}||L+S-M||_{F}
\end{eqnarray}


It is remarked that the $M$ is a perturbed observation outcome of the original $(L_{0},S_{0})$ pairs.


%------------------------------------------------------------------------------
\subsubsection{Contributions}
\begin{enumerate}
\item It provides sufficient conditions on sparsity of the original $(L_{0},S_{0})$ pairs that allow accurate recovery in the sense that $||L_{0}-\hat{L}||_{\infty},||S_{0}-\hat{S}||_{\infty}$ is small.
\item If the observed matrix $M$ is pertured from $L_{0}+S_{0}$ by a small amount (i.e.$\epsilon$), the optimizer $(\hat{L}, \hat{S})$ will be $\epsilon-$close to the orginal $(L_{0},S_{0})$ pairs.
\end{enumerate}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Characterization of the subdifferential of some matrix norms : GA Watson et. al.}


%------------------------------------------------------------------------------
\subsubsection{Significance}

Subdifferential of nuclear norm is a critical element in the formulation of dual certificates. This paper proves why the subdifferential of nuclear norm is of the form.

\[
\{U^{(1)}V^{(1)T}+W|U^{(1)T}W=0,WV^{(1)},||W||_{2}\le1\}
\]


where $A=[U^{(1)}|U^{(2)}]
[
\begin{array}{cc}
\Sigma & 0\\
0 & 0
\end{array}
]
[
\begin{array}{c}
V^{(1)T}\\
V^{(2)T}
\end{array}
]
$


%------------------------------------------------------------------------------
\subsubsection{Details}

\begin{lemma}
If a norm $||\cdot||$ is orthogonal invariant and is characterized by a symmetric gauge function, then
\textup{
\[
\partial||A||=conv\{Udiag(\vec{d})V|A=U[\begin{array}{cc}
\Sigma & 0\\
0 & 0
\end{array}]V^{T},\vec{d}\in\partial\phi(\Sigma)\}
\]
}
\end{lemma}

\begin{proof}
Sketch: For $RHS\subset LHS$, one can check that they satisfy the dual norm characterization of $\partial||A||$. For $LHS\subset RHS$, one can use maximum representation of the directional derivative of
the $||A||$ to show that subdifferential not inside RHS will give
a contradiction. 
\end{proof}

\begin{prop}
\[
\partial||A||_{*}=\{U^{(1)}V^{(1)T}+W|U^{(1)T}W=0,WV^{(1)},||W||_{2}\le1\}
\]
\end{prop}

\begin{proof}
Sketch: Take $\phi(\vec{\sigma}) =  \|\sigma \|_1$ and apply Lemma~\ref{} to the convex hull of all SVD of $A$.
\end{proof}



%============================================================================================
\section{Generalization of the Robust PCA scheme with known rank}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Introduction }

Recall that the nuclear norm introduced in the PCP scheme is resulted because we would like to extract the low rank component from gross random noise. Nuclear norm is used because it is a heuristic for penalizing high rank matrix. Now, we consider the case when we have extra information/guess from the data set that we know precisely what the rank of the matrix are. Therefore, it is natural to introduce the following the following heuristics. 

\begin{eqnarray*}
E^{*} & = & \min_{\{p_{i}\}\{q_{j}\},||p_{j}||_{2},1\le j\le r}||M-\sum_{j=1}^{r}p_{j}q_{j}^{T}||_{1}
\end{eqnarray*}


We specialize the discussion to the case of rank-1 approximation (when $r=1$) and shows that this scheme has a  probabilistic gurantee of recovery and demonstrate that an efficient power iteration can be employed to solve this problem. Empirical results are also provided to support this method. We remark that we can generalize this and obtain similar results. 


%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Problem statement.}

We are now interested in solving the following problem. 

\begin{eqnarray}
E^{*} & = & \min_{p,q}||M-pq^{T}||_{1}\label{rankonemin}
\end{eqnarray}



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Algorithm derivation}

Let $M=(a_{i,j})\in R^{mXn}$. We now employ the block-coordinate descent method to solve this problem. Note that 

\begin{eqnarray}
\min_{p} & ||M-pq^{T}|| & =\sum_{i=1}^{m}\min_{t}(\sum_{j=1}^{n}|a_{i,j}-tq_{j}|)\\
 &  & =\sum_{i=1}^{m}\min_{t}(\sum_{j=1}^{n}|q_{j}||t-\frac{a_{i,j}}{q_{j}}|)\nonumber 
\end{eqnarray}
\begin{eqnarray}
\min_{q} & ||M-pq^{T}|| & =\sum_{j=1}^{n}\min_{t}(\sum_{i=1}^{m}|a_{i,j}-tp_{i}|)\\
 &  & =\sum_{j=1}^{n}\min_{t}(\sum_{j=1}^{n}|p_{i}||t-\frac{a_{i,j}}{p_{i}}|)\nonumber 
\end{eqnarray}


And for solving the subproblem of finding 
\[
\min_{t}\sum_{k=1}^{k_{0}}c_{i}|t-d_{i}|
\]
where $c_{i}\ge0$ is basically finding the weighted median and can be done by the following method with complexity $O(k_{0}\log k_{0})$ mostly on sorting the sequence. We call it WMH.

\begin{algorithm}[tbh]
\begin{enumerate}
\item We first sort $\vec{\ensuremath{d}}$ s.t. $d_{i_{1}}\le d_{i_{2}}\le...\le d_{i_{k_{0}}}$ 
\item We then find $k^{'}$s.t. 
\begin{eqnarray*}
\sum_{\theta=1}^{k^{'}-1}c_{i_{\theta}} & \le & \sum_{\theta=k^{'}}^{k_{0}}c_{i_{\theta}}\\
\sum_{i=1}^{k^{'}}c_{i_{\theta}} & \ge & \sum_{i=k^{'}+1}^{k_{0}}c_{i_{\theta}}
\end{eqnarray*}

\item We then set $t^{*}$to be $d_{i_{k^{'}}}$
\end{enumerate}
\caption{WMH $(k_{0},\vec{c},\vec{d})$}
\end{algorithm}


This algorithm is optimal in finding $t$. This is justified by using the property of sub-differential of $||\cdot||_{1}$ and note that $0\in\partial(\sum_{k=1}^{k_{0}}c_{i}|t^{*}-d_{i}|)$. 

Now we are ready to state the power iteration method to solve the optimization rank-1 optimization problem. We call it Poweriteration.

\begin{algorithm}[tbh]
Repeat
\begin{enumerate}
\item $p_{i}\leftarrow wmh(n,abs(q),M(i,:)./q)$ for each i
\item $p\leftarrow\frac{p}{max(p)}$
\item $q_{j}\leftarrow wmh(n,abs(p),M(:,j)./p)$ for each j
\end{enumerate}
Until stopping criterion is met

\caption{Poweriteration($M$)}
\end{algorithm}



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Performance Guarantee}

Here are two results about performance guarantee. One is a result for deterministic case and the other is for the random case. 
\begin{prop}
Let $M=pq^{T}+S$ with $\frac{2}{\epsilon}||S||_{1}<||pq^{T}||_{1}$. Then the $(\hat{p},\hat{q})$ recovered from (\ref{rankonemin})
satisfy 
\begin{eqnarray*}
\frac{||pq^{T}-\hat{p}\hat{q}^{T}||}{||pq^{T}||_{1}} & \le & \epsilon
\end{eqnarray*}
\end{prop}
\begin{proof}
If $(\hat{p},\hat{q})=(p,q)$ then the proposition is trivially true.
Now assume $(\hat{p},\hat{q})\not=(p,q)$. Then we have ,

\begin{eqnarray*}
||S||_{1} & \ge & ||pq^{T}+S-\hat{p}\hat{q}^{T}||_{1}\\
 & \ge & ||pq^{T}-\hat{p}\hat{q}^{T}||_{1}-||S||_{1}
\end{eqnarray*}


Now if the conclusion is false, we will have 
\begin{eqnarray*}
||S||_{1} & \ge & \epsilon||pq^{T}||_{1}-||S||_{1}
\end{eqnarray*}


This gives a contradiction of, 
\begin{eqnarray*}
2||S||_{1} & \ge & \epsilon||pq^{T}||_{1}
\end{eqnarray*}
\end{proof}
\begin{prop}
Let $M=pq^{T}+S$ be $R^{nXn}$ and $S$ has $k$ possibly non-zero
elements. If $S_{i}\sim U[-x_{s},x_{s}]$, $p_{i}\sim U[-x_{p},x_{p}]$,$q_{i}\sim U[-x_{p},x_{p}]$
independently distributed, with $\frac{n}{k}\ge\frac{8x_{s}}{\epsilon x_{p}x_{q}}$,
then 
\begin{eqnarray*}
\lim_{n\to\infty}Pr(\frac{||pq^{T}-\hat{p}\hat{q}^{T}||}{||pq^{T}||_{1}}>\epsilon) & = & 0
\end{eqnarray*}
\end{prop}
\begin{proof}
Let the event that $\frac{||pq^{T}-\hat{p}\hat{q}^{T}||}{||pq^{T}||_{1}}>\epsilon$
be an error event. If error occurs, then 

\begin{eqnarray*}
kx_{s} & \ge & ||pq^{T}-\hat{p}\hat{q}^{T}+S||_{1}\\
 & \ge & ||pq^{T}-\hat{p}\hat{q}^{T}||_{1}-||S||_{1}\\
 & \ge & \epsilon||pq^{T}||_{1}-kx_{s}
\end{eqnarray*}


We now bounding the probability of error. 
\begin{eqnarray*}
 &  & Pr(\frac{||pq^{T}-\hat{p}\hat{q}^{T}||}{||pq^{T}||_{1}}>\epsilon)\\
 & \le & Pr(kx_{s}\ge\epsilon||pq^{T}||_{1}-kx_{s})\\
 & \le & Pr(\frac{2kx_{s}}{\epsilon}\ge||pq^{T}||_{1})\\
 & \le & (1-(\frac{t}{x_{q}})^{n})Pr(\frac{1}{n}\sum_{i=1}^{n}|p_{i}|\le\frac{2kx_{s}}{tn\epsilon})+(\frac{t}{x_{q}})^{n}
\end{eqnarray*}


Then by Chebyshev inequality, we have, by putting $t=\frac{1}{2x_{q}}$
\begin{eqnarray*}
 &  & Pr(\frac{||pq^{T}-\hat{p}\hat{q}^{T}||}{||pq^{T}||_{1}}>\epsilon)\\
 & \le & (1-(\frac{1}{2})^{n})\frac{\frac{1}{n}(\frac{x_{p}^{2}}{12})}{(\frac{x_{p}}{2}-\frac{2kx_{s}}{n\epsilon x_{q}})}+(\frac{1}{2})^{n}
\end{eqnarray*}


And this goes to zero as $n\to\infty$.
\end{proof}



%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Empirical Evidence}

We simulate the power iteration in recovering sparse noise. In this simulation, we use randomly generated the entries of $p$ and $q$ as $N(0,1)$ iid. And then we randomly generate sparse matrix with sparse support uniformly distributed across the nXn matrix. And each sparse entry has a value with distribution of $N(0,1)$. We then plot the graph of different degree of sparsity and the corresponding effectiveness of the optimization heuristic in extracting the original $pq^{T}$.
The result is as follows.


\begin{figure}[tbh]
%\includegraphics[width=6in]{\string"C:/Users/kk/Desktop/EE227A/project/matlab experiments/fastalgo\string".jpg}
\caption{Simulation result}
\end{figure}






%============================================================================================
\section{Algorithms}


%============================================================================================
\section{Applications}





%============================================================================================
\newpage
\bibliographystyle{abbrv}
\bibliography{../common/RobustPCA}

\end{document}